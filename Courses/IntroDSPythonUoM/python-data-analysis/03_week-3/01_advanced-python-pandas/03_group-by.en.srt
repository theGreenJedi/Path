1
00:00:09,478 --> 00:00:13,042
We've seen that even though PANDAS allows
us to iterate over every row in a data

2
00:00:13,042 --> 00:00:15,904
frame this is generally a slow way
to accomplish a given task and

3
00:00:15,904 --> 00:00:17,950
it's not very pandorable.

4
00:00:17,950 --> 00:00:20,868
For instance, if we wanted to write
some code to iterate over all the of

5
00:00:20,868 --> 00:00:23,944
the states and generate a list of
the average census population numbers.

6
00:00:23,944 --> 00:00:27,370
We could do so
using a loop in the unique function.

7
00:00:27,370 --> 00:00:30,710
Another option is to use
the dataframe group by function.

8
00:00:30,710 --> 00:00:35,137
This function takes some column name or
names and splits the dataframe up into

9
00:00:35,137 --> 00:00:39,170
chunks based on those names,
it returns a dataframe group by object.

10
00:00:39,170 --> 00:00:40,825
Which can be iterated upon, and

11
00:00:40,825 --> 00:00:44,197
then returns a tuple where the first
item is the group condition,

12
00:00:44,197 --> 00:00:48,100
and the second item is the data
frame reduced by that grouping.

13
00:00:48,100 --> 00:00:50,940
Since it's made up of two values,
you can unpack this, and

14
00:00:50,940 --> 00:00:54,220
project just the column that you're
interested in, to calculate the average.

15
00:00:56,290 --> 00:00:59,090
Here's some examples of
both methods in action.

16
00:00:59,090 --> 00:01:04,442
Let's first load the census data, then
exclude the state level summarizations,

17
00:01:04,442 --> 00:01:06,741
which had a sum level value of 40.

18
00:01:06,741 --> 00:01:09,905
Here's some examples of
both methods in action.

19
00:01:09,905 --> 00:01:13,911
Let's first load the census data,
then exclude the state level

20
00:01:13,911 --> 00:01:16,815
summarizations which
had a sum level of 40.

21
00:01:16,815 --> 00:01:19,135
In the first we used the census date.

22
00:01:19,135 --> 00:01:21,075
We get a list of the unique states.

23
00:01:21,075 --> 00:01:25,355
Then for each state we reduce the data
frame and calculate the average.

24
00:01:26,595 --> 00:01:28,695
If we time this we see it takes a while.

25
00:01:28,695 --> 00:01:31,605
I've set the number of loops here
the time it should take to ten

26
00:01:31,605 --> 00:01:32,465
because I'm live loading.

27
00:01:33,780 --> 00:01:36,800
Here's the same approach
with a group by object.

28
00:01:36,800 --> 00:01:40,210
We tell pandas we're interested in
group and with a state name and then we

29
00:01:40,210 --> 00:01:44,780
calculate the average using just one
column and all of the data in that column.

30
00:01:44,780 --> 00:01:47,100
When we time it we see a huge
difference in the speed

31
00:02:04,196 --> 00:02:08,248
Now, 99% of the time, you'll use
group by on one or more columns.

32
00:02:08,248 --> 00:02:11,375
But you can actually provide
a function to group by as well and

33
00:02:11,375 --> 00:02:13,760
use that to segment your data.

34
00:02:13,760 --> 00:02:15,910
This is a bit of a fabricated example but

35
00:02:15,910 --> 00:02:19,430
lets say that you have a big batch
job with lots of processing and

36
00:02:19,430 --> 00:02:23,210
you want to work on only a third or
so of the states at a given time.

37
00:02:23,210 --> 00:02:26,130
We could create some function which
returns a number between zero and

38
00:02:26,130 --> 00:02:29,530
two based on the first
character of the state name.

39
00:02:29,530 --> 00:02:33,300
Then we can tell group by to use this
function to split up our data frame.

40
00:02:33,300 --> 00:02:36,250
It's important to note that in order
to do this you need to set the index of

41
00:02:36,250 --> 00:02:38,970
the data frame to be the column
that you want to group by first.

42
00:02:40,810 --> 00:02:41,930
Here's an example.

43
00:02:41,930 --> 00:02:44,301
We'll create some new
function called fun and

44
00:02:44,301 --> 00:02:47,840
if the first letter of the parameter
is a capital M we'll return a 0.

45
00:02:47,840 --> 00:02:52,010
If it's a capital Q we'll return a 1 and
otherwise we'll return a 2.

46
00:02:52,010 --> 00:02:54,620
Then we'll pass this function
to the data frame reply, and

47
00:02:54,620 --> 00:02:58,470
see that the data frame is segmented
by the calculated group number.

48
00:02:58,470 --> 00:03:01,753
This kind of technique,
which is sort of a light weight hashing,

49
00:03:01,753 --> 00:03:04,989
is commonly used to distribute
tasks across multiple workers.

50
00:03:04,989 --> 00:03:09,690
Whether they are cores in a processor,
nodes in a supercomputer, or

51
00:03:09,690 --> 00:03:11,156
disks in a database.

52
00:03:11,156 --> 00:03:14,381
A common work flow with group
bias that you split your data,

53
00:03:14,381 --> 00:03:17,540
you apply some function,
then you combine the results.

54
00:03:17,540 --> 00:03:19,548
This is called split
apply combine pattern.

55
00:03:19,548 --> 00:03:22,840
And we've seen the splitting method,
but what about apply?

56
00:03:22,840 --> 00:03:25,900
Certainly iterative methods as
we've seen can do this, but

57
00:03:25,900 --> 00:03:30,890
the groupby object also has a method
called agg which is short for aggregate.

58
00:03:30,890 --> 00:03:33,030
This method applies
a function to the column or

59
00:03:33,030 --> 00:03:35,460
columns of data in the group,
and returns the results.

60
00:03:36,600 --> 00:03:40,650
With agg, you simply pass in a dictionary
of the column names that you're interested

61
00:03:40,650 --> 00:03:43,140
in, and
the function that you want to apply.

62
00:03:43,140 --> 00:03:46,720
For instance to build a summary data frame
for the average populations per state,

63
00:03:46,720 --> 00:03:51,304
we could just give agg a dictionary
with the Census 2010 pop key and

64
00:03:51,304 --> 00:03:59,110
the numb/pie average function.

65
00:04:03,496 --> 00:04:05,393
Now, I want to flag a potential issue and

66
00:04:05,393 --> 00:04:07,860
using the aggregate method
of group by objects.

67
00:04:07,860 --> 00:04:10,930
You see, when you pass in a dictionary
it can be used to either to identify

68
00:04:10,930 --> 00:04:13,160
the columns to apply a function on or

69
00:04:13,160 --> 00:04:16,980
to name an output column if there's
multiple functions to be run.

70
00:04:16,980 --> 00:04:20,950
The difference depends on the keys that
you pass in from the dictionary and

71
00:04:20,950 --> 00:04:21,520
how they're named.

72
00:04:22,570 --> 00:04:26,230
In short, while much of the documentation
and examples will talk about a single

73
00:04:26,230 --> 00:04:29,190
groupby object,
there's really two different objects.

74
00:04:29,190 --> 00:04:31,584
The data frame groupby and
the series groupby.

75
00:04:31,584 --> 00:04:34,499
And these objects behave a little
bit differently with aggregate.

76
00:04:35,580 --> 00:04:39,700
For instance, we take our census data and
convert it into a series with the state

77
00:04:39,700 --> 00:04:44,110
names as the index and only columns
as the census 2010 population.

78
00:04:44,110 --> 00:04:47,860
And then we can group this by
index using the level parameter.

79
00:04:47,860 --> 00:04:50,800
Then we call the agg method where
the dictionary that has both the numpie

80
00:04:50,800 --> 00:04:53,445
average and the numpie sum functions.

81
00:04:53,445 --> 00:04:57,317
PANDAS applies those functions to the
series object and, since there's only one

82
00:04:57,317 --> 00:05:01,145
column of data It apples both functions
to that column and prints out the output.

83
00:05:14,743 --> 00:05:17,750
We can do the same thing with
a data frame instead of a series.

84
00:05:17,750 --> 00:05:20,890
We set the index to be the state name,
we group by the index, and

85
00:05:20,890 --> 00:05:22,540
we project two columns.

86
00:05:22,540 --> 00:05:26,830
The population estimate in 2010,
the population estimate in 2011.

87
00:05:26,830 --> 00:05:28,969
When we call aggregate
with two parameters,

88
00:05:28,969 --> 00:05:33,670
it builds a nice hierarchical column space
and all of our functions are applied.

89
00:05:33,670 --> 00:05:37,630
Where the confusion can come in is when
we change the labels of the dictionary we

90
00:05:37,630 --> 00:05:41,980
passed to aggregate, to correspond to
the labels in our group data frame.

91
00:05:41,980 --> 00:05:46,050
In this case, pandas recognizes that
they're the same and maps the functions

92
00:05:46,050 --> 00:05:50,620
directly to columns instead of creating
a hierarchically labeled column.

93
00:05:50,620 --> 00:05:52,980
From my perspective this
is very odd behavior,

94
00:05:52,980 --> 00:05:55,770
not what I would expect
given the labeling change.

95
00:05:55,770 --> 00:05:58,510
So just be aware of this when
using the aggregate function.

96
00:05:59,690 --> 00:06:01,360
So that's the group by function.

97
00:06:01,360 --> 00:06:05,390
I use the group by function regularly
in my work, and it's very handy for

98
00:06:05,390 --> 00:06:08,650
segmenting a data frame,
working on small pieces of the data frame,

99
00:06:08,650 --> 00:06:10,980
and then creating bigger
data frames later.