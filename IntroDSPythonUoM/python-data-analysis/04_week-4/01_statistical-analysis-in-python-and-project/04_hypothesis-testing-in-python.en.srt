1
00:00:08,965 --> 00:00:12,550
We use statistics in a lot of
different ways in data science.

2
00:00:12,550 --> 00:00:17,019
In this lecture, I want to refresh
your knowledge of hypothesis testing,

3
00:00:17,019 --> 00:00:20,995
which is a core data analysis
activity behind experimentation.

4
00:00:20,995 --> 00:00:25,157
We're starting to see experimentation
used more and more commonly

5
00:00:25,157 --> 00:00:29,838
outside of academic scientific, and
in day to day business environments.

6
00:00:29,838 --> 00:00:34,510
Part of the reason for this is
the rise of big data and web commerce.

7
00:00:34,510 --> 00:00:37,300
It's now easy to change
your digital storefront and

8
00:00:37,300 --> 00:00:40,620
deliver a different experience
to some of your customers, and

9
00:00:40,620 --> 00:00:44,560
then see how those customer actions
might differ from one another.

10
00:00:44,560 --> 00:00:49,298
For instance, if you sell books, you might
want to have one condition where the cover

11
00:00:49,298 --> 00:00:51,565
of the book is featured
on the web page and

12
00:00:51,565 --> 00:00:55,984
another condition where the focus is on
the author and the reviews of the book.

13
00:00:55,984 --> 00:00:58,382
This is often called A/B testing.

14
00:00:58,382 --> 00:01:03,246
And while it's not unique to this time in
history, it's now becoming so common that

15
00:01:03,246 --> 00:01:07,918
if you're using a website, you are
undoubtedly part of an A/B test somewhere.

16
00:01:07,918 --> 00:01:12,378
This raises some interesting ethical
questions and I've added a reading to

17
00:01:12,378 --> 00:01:16,558
the course resources and I would like
to encourage you to check it out and

18
00:01:16,558 --> 00:01:20,410
join in the discussion, but
let's refocus back on statistics.

19
00:01:21,470 --> 00:01:24,347
A hypothesis is a statement
that we can test.

20
00:01:24,347 --> 00:01:28,542
I'll pull an example from my own
research of educational technology and

21
00:01:28,542 --> 00:01:29,858
learning analytics.

22
00:01:29,858 --> 00:01:34,567
Let's say that we have an expectation that
when a new course is launched on a MOOC

23
00:01:34,567 --> 00:01:38,739
platform, the keenest students find
out about it and all flock to it.

24
00:01:38,739 --> 00:01:42,507
Thus, we might expect that those
students who sign up quite quickly after

25
00:01:42,507 --> 00:01:45,487
the course is launched with
higher performance than those

26
00:01:45,487 --> 00:01:48,850
students who signed up after the MOOC
has been around for a while.

27
00:01:48,850 --> 00:01:53,469
In this example, we have samples from two
different groups which we want to compare.

28
00:01:53,469 --> 00:01:55,740
The early sign ups and the late sign ups.

29
00:01:56,940 --> 00:02:02,980
When we do hypothesis testing, we hold out
that our hypothesis as the alternative and

30
00:02:02,980 --> 00:02:07,010
we create a second hypothesis
called the null hypothesis,

31
00:02:07,010 --> 00:02:11,100
which in this case would be that there
is no difference between groups.

32
00:02:11,100 --> 00:02:14,820
We then examine the groups to determine
whether this null hypothesis is

33
00:02:14,820 --> 00:02:15,430
true or not.

34
00:02:16,560 --> 00:02:19,371
If we find that there is
a difference between groups,

35
00:02:19,371 --> 00:02:23,064
then we can reject the null hypothesis and
we accept our alternative.

36
00:02:23,064 --> 00:02:25,035
There are subtleties in this description.

37
00:02:25,035 --> 00:02:28,553
We aren't saying that our
hypothesis is true per se, but

38
00:02:28,553 --> 00:02:32,822
we're saying that there's evidence
against the null hypothesis.

39
00:02:32,822 --> 00:02:37,396
So, we're more confident in
our alternative hypothesis.

40
00:02:37,396 --> 00:02:39,685
Let's see an example of this.

41
00:02:39,685 --> 00:02:42,885
We can load a file called grids.csv.

42
00:02:42,885 --> 00:02:45,008
If we take a look at
the data frame inside,

43
00:02:45,008 --> 00:02:47,203
we see we have six different assignments.

44
00:02:47,203 --> 00:02:49,258
Each with a submission time and

45
00:02:49,258 --> 00:02:53,853
it looks like there just under
3,000 entries in this data file.

46
00:02:53,853 --> 00:02:58,470
For the purpose of this lecture, let's
segment this population in to two pieces.

47
00:02:58,470 --> 00:03:01,650
Those who finish the first assignment
by the end of December 2015 and

48
00:03:01,650 --> 00:03:04,340
those who finish it sometimes after that.

49
00:03:05,470 --> 00:03:08,120
I just made this date up and
it gives us two data frames,

50
00:03:08,120 --> 00:03:09,530
which are roughly the same size.

51
00:03:11,010 --> 00:03:14,770
As you've seen, the pandas data frame
object has a variety of statistical

52
00:03:14,770 --> 00:03:17,210
functions associated with it.

53
00:03:17,210 --> 00:03:19,860
If we call the mean function
directly on the data frame,

54
00:03:19,860 --> 00:03:22,579
we see that each of the means for
the assignments are calculated.

55
00:03:23,640 --> 00:03:28,488
Note that the date time values are ignored
as panda's knows this isn't a number,

56
00:03:28,488 --> 00:03:29,692
but an object type.

57
00:03:29,692 --> 00:03:33,165
If we look at the mean values for
the late data frame as well,

58
00:03:33,165 --> 00:03:35,468
we get surprisingly similar numbers.

59
00:03:35,468 --> 00:03:37,180
There are slight differences, though.

60
00:03:37,180 --> 00:03:39,430
It looks like the end
of the six assignments,

61
00:03:39,430 --> 00:03:42,400
the early users are doing better
by about a percentage point.

62
00:03:43,400 --> 00:03:47,100
So, is this enough to go ahead and make
some interventions to actually try and

63
00:03:47,100 --> 00:03:50,180
change something in the way we teach?

64
00:03:50,180 --> 00:03:55,370
When doing hypothesis testing, we have to
choose a significance level as a threshold

65
00:03:55,370 --> 00:03:58,930
for how much of a chance
we're willing to accept.

66
00:03:58,930 --> 00:04:02,140
This significance level is
typically called alpha.

67
00:04:02,140 --> 00:04:05,890
It can vary greatly, depending on what
you're going to do with the result and

68
00:04:05,890 --> 00:04:07,850
the amount of noise you
expect in your data.

69
00:04:08,870 --> 00:04:13,221
For instance, in social sciences research,
a value of 0.05 or

70
00:04:13,221 --> 00:04:17,940
0.01 is often used,
which indicates a tolerance for

71
00:04:17,940 --> 00:04:22,350
a probability of between 5% and
1% of chance.

72
00:04:22,350 --> 00:04:25,490
In a physics experiment where the
conditions are much more controlled and

73
00:04:25,490 --> 00:04:29,320
thus, the burden of proof is much higher,
you might expect to see alpha

74
00:04:29,320 --> 00:04:33,590
levels of 10 to the negative 5 or
100,000th of a percentage.

75
00:04:35,490 --> 00:04:40,280
You can think of the significance level
from the perspective of interventions as

76
00:04:40,280 --> 00:04:44,165
well and this is something I run
into regularly with my research.

77
00:04:44,165 --> 00:04:48,637
What am I going to do when I find out that
two student populations are different?

78
00:04:48,637 --> 00:04:52,076
For instance, if I'm going to send
an email nudge to encourage students to

79
00:04:52,076 --> 00:04:56,720
continue working on their homework,
that's a pretty low-cost intervention.

80
00:04:56,720 --> 00:05:00,540
Emails are cheap and while I certainly
don't want to annoy students,

81
00:05:00,540 --> 00:05:03,140
one extra email isn't
going to ruin their day.

82
00:05:03,140 --> 00:05:06,101
But what if the intervention
is a little more involved,

83
00:05:06,101 --> 00:05:09,885
like having our tutorial assistant
followup with a student via phone?

84
00:05:09,885 --> 00:05:13,189
This is all of a sudden much more
expensive for both the institution and for

85
00:05:13,189 --> 00:05:13,834
the student.

86
00:05:13,834 --> 00:05:16,430
So, I might want to ensure
a higher burden of proof.

87
00:05:17,530 --> 00:05:19,187
So the threshold you set for

88
00:05:19,187 --> 00:05:22,868
alpha depends on what you might
do with the result, as well.

89
00:05:22,868 --> 00:05:29,181
For this example, let's use a threshold
of 0.05 for our alpha or 5%.

90
00:05:29,181 --> 00:05:33,693
Now, how do we actually test whether
these means are different in Python?

91
00:05:33,693 --> 00:05:37,782
The SciPy library contains a number
of different statistical tests and

92
00:05:37,782 --> 00:05:40,584
forms a basis for
hypothesis testing in Python.

93
00:05:40,584 --> 00:05:45,060
A T test is one way to compare
the means of two different populations.

94
00:05:45,060 --> 00:05:48,444
In the SciPy library,
the T test end function will

95
00:05:48,444 --> 00:05:53,210
compare two independent samples to
see if they have different means.

96
00:05:53,210 --> 00:05:56,721
I'm not going to go into the details
of any of this statistical test here.

97
00:05:56,721 --> 00:06:01,247
But instead, we'd recommend that you check
out the Wikipedia page on particular test

98
00:06:01,247 --> 00:06:06,072
or consider taking a full statistics
course if this is unfamiliar to you.

99
00:06:06,072 --> 00:06:09,637
But I do want to note that most
statistical tests expect that

100
00:06:09,637 --> 00:06:13,071
the data conforms to a certain
distribution, a shape.

101
00:06:13,071 --> 00:06:19,098
So, you shouldn't apply such tests blindly
and should investigate your data first.

102
00:06:19,098 --> 00:06:21,649
If we want to compare
the assignment grades for

103
00:06:21,649 --> 00:06:26,012
the first assignment between the two
populations, we could generate a T test

104
00:06:26,012 --> 00:06:29,182
by passing these two series
into the T test in function.

105
00:06:29,182 --> 00:06:33,144
The result is a two with a test
statistic and a p-value.

106
00:06:33,144 --> 00:06:36,887
The p-value here is much
larger than our 0.05.

107
00:06:36,887 --> 00:06:39,591
So we cannot reject the null hypothesis,

108
00:06:39,591 --> 00:06:42,690
which is that the two
populations are the same.

109
00:06:42,690 --> 00:06:45,349
In more late terms,
we would say that there's

110
00:06:45,349 --> 00:06:49,721
no statistically significant difference
between these two sample means.

111
00:06:49,721 --> 00:06:51,413
Let's check with assignment two grade.

112
00:06:58,037 --> 00:07:01,118
No, that's much larger than 0.052.

113
00:07:01,118 --> 00:07:02,639
How about with assignment three?

114
00:07:08,857 --> 00:07:13,191
Well, that's much closer, but
still beyond our threshold value.

115
00:07:13,191 --> 00:07:17,832
It's important to stop here and talk
about serious process from with how we're

116
00:07:17,832 --> 00:07:22,557
handling this investigation of the
difference between these two populations.

117
00:07:22,557 --> 00:07:27,424
When we set the alpha to be 0.05,
we're saying that we expect it that there

118
00:07:27,424 --> 00:07:31,280
will be positive result,
5% of the time just to the chance.

119
00:07:32,440 --> 00:07:36,880
As we run more and more T tests, we're
more likely to find a positive result

120
00:07:36,880 --> 00:07:39,250
just because of the number
of T tests we have run.

121
00:07:40,340 --> 00:07:44,853
When a data scientist run many tests
in this way, it's called p-hacking or

122
00:07:44,853 --> 00:07:48,039
dredging and
it's a serious methodological issue.

123
00:07:48,039 --> 00:07:54,875
P-hacking results in spurious correlations
instead of generalizable results.

124
00:07:54,875 --> 00:07:57,684
There are a couple of different
ways you can deal with p-hacking.

125
00:07:57,684 --> 00:08:00,490
The first is called
the Bonferroni correction.

126
00:08:00,490 --> 00:08:03,187
In this case,
you simply tighten your alpha value,

127
00:08:03,187 --> 00:08:07,187
the threshold of significance based on
the number of tests you're running.

128
00:08:07,187 --> 00:08:12,130
So if you choose 0.05 with 1 test,
you want to run 3 test,

129
00:08:12,130 --> 00:08:19,406
you reduce alpha by multiplying 0.05 by
one-third to get a new value of 0.01 sub.

130
00:08:19,406 --> 00:08:23,400
I personally find this approach
to be very conservative.

131
00:08:23,400 --> 00:08:26,639
Another option is to hold
out some of your data for

132
00:08:26,639 --> 00:08:29,799
testing to see how
generizable your result is.

133
00:08:29,799 --> 00:08:32,550
In this case,
we might take half of our data for

134
00:08:32,550 --> 00:08:35,741
each of the two data frames,
run our T test with that.

135
00:08:35,741 --> 00:08:39,435
Form specific hypothesis based
on the result of these tests,

136
00:08:39,435 --> 00:08:42,500
then run very limited tests
on the rest of the data.

137
00:08:43,540 --> 00:08:46,400
This method is actually heavily
used in machine learning when

138
00:08:46,400 --> 00:08:50,410
building predictive models where it's
called cross fold validation and

139
00:08:50,410 --> 00:08:53,740
you'll learn more about this in
third course in this specialization.

140
00:08:55,270 --> 00:08:59,240
A final method which has come about is
the pre-registration of your experiment.

141
00:09:00,400 --> 00:09:04,060
In this step, you would outline what
you expect to find and why, and

142
00:09:04,060 --> 00:09:08,120
describe the test that would
backup a positive proof of this.

143
00:09:08,120 --> 00:09:12,150
You register it with a third party, in
academic circles, this is often a journal

144
00:09:12,150 --> 00:09:14,330
who determines whether it's
a reasonable test or rather not.

145
00:09:15,360 --> 00:09:17,707
You then run your study and
report the results,

146
00:09:17,707 --> 00:09:20,299
regardless as to whether
they were positive or not.

147
00:09:20,299 --> 00:09:25,060
Here, there is a larger burden on
connecting to existing theory since

148
00:09:25,060 --> 00:09:29,661
you need to convince reviewers that
the experiment is likely to test

149
00:09:29,661 --> 00:09:31,531
fully a given hypothesis.

150
00:09:31,531 --> 00:09:32,611
In this lecture,

151
00:09:32,611 --> 00:09:37,147
we've discussed just some of the basics
of hypothesis testing in Python.

152
00:09:37,147 --> 00:09:41,421
I introduced you to the SciPy library,
which you can use for T testing.

153
00:09:41,421 --> 00:09:44,384
We've discussed some of the practical
issues which arise from looking for

154
00:09:44,384 --> 00:09:46,420
statistical significance.

155
00:09:46,420 --> 00:09:49,240
There's much more to learn
about hypothesis testing.

156
00:09:49,240 --> 00:09:51,120
For instance,
there are different tests used,

157
00:09:51,120 --> 00:09:53,800
depending on the shape of your data and
different ways to report

158
00:09:53,800 --> 00:09:57,610
results instead of just p-values
such as confidence intervals.

159
00:09:57,610 --> 00:10:00,450
But I hope this gives you a start
to comparing the means of two

160
00:10:00,450 --> 00:10:03,772
different populations, which is
a common task for data scientists and

161
00:10:03,772 --> 00:10:07,239
we'll followup some of this work in
the second course in this series.

162
00:10:09,320 --> 00:10:11,731
This lecture also
completes the lectures for

163
00:10:11,731 --> 00:10:15,735
the first course in the Applied Data
Science with Python Specialization.

164
00:10:15,735 --> 00:10:18,350
We've covered the basics
of Python programming,

165
00:10:18,350 --> 00:10:22,199
some more advanced features like maps,
lambdas and miscomprehensions.

166
00:10:22,199 --> 00:10:26,827
How to read and manipulate data using
the panda's library, including querying,

167
00:10:26,827 --> 00:10:31,538
joining, grouping and processing of data
frames and the creation of pivot tables.

168
00:10:31,538 --> 00:10:35,398
And now we've talked a little bit
about statistics in Python and

169
00:10:35,398 --> 00:10:38,274
dug deeper into the Num-Py and
SciPy toolkit.

170
00:10:38,274 --> 00:10:41,717
In the next course, we'll dig into
plotting and charting of data.

171
00:10:41,717 --> 00:10:45,056
Dealing a bit more with statistics and
how we present data to others, and

172
00:10:45,056 --> 00:10:48,160
how we build a compelling story for
the data we have.

173
00:10:48,160 --> 00:10:48,690
I'll see you then.