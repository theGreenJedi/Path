1
00:00:08,895 --> 00:00:13,571
Many of the distributions you use in data
science are not discrete binomial, and

2
00:00:13,571 --> 00:00:17,631
instead are continues where the value
of the given observation isn't

3
00:00:17,631 --> 00:00:21,852
a category like heads or tails, but
can be represented by a real number.

4
00:00:21,852 --> 00:00:25,959
It's common to then graph these
distributions when talking about them,

5
00:00:25,959 --> 00:00:28,741
where the x axis is the value
of the observation and

6
00:00:28,741 --> 00:00:33,000
the y axis represents the probability
that a given observation will occur.

7
00:00:34,030 --> 00:00:38,040
If all numbers are equally likely to
be drawn when you sample from it,

8
00:00:38,040 --> 00:00:41,050
this should be graphed as
a flat horizontal line.

9
00:00:41,050 --> 00:00:44,519
And this flat line is actually
called the uniform distribution.

10
00:00:45,670 --> 00:00:48,910
There are few other distributions
that get a lot more interesting.

11
00:00:48,910 --> 00:00:52,910
Let's take the normal distribution which
is also called Gaussian Distribution or

12
00:00:52,910 --> 00:00:54,470
sometimes, a Bell Curve.

13
00:00:55,570 --> 00:00:58,490
This distribution looks like
a hump where the number which has

14
00:00:58,490 --> 00:01:01,650
the highest probability of
being drawn is a zero, and

15
00:01:01,650 --> 00:01:05,030
there are two decreasing curves
on either side of the X axis.

16
00:01:06,250 --> 00:01:09,930
One of the properties of this
distribution is that the mean is zero,

17
00:01:09,930 --> 00:01:12,360
not the two curves on
either side are symmetric.

18
00:01:12,360 --> 00:01:16,540
I want to introduce you to
the term expected value.

19
00:01:16,540 --> 00:01:20,210
I think that most of us are familiar with
the mean is the sum of all the values

20
00:01:20,210 --> 00:01:22,790
divided by the total number of values.

21
00:01:22,790 --> 00:01:26,166
Calculating a mean values
are computational process, and

22
00:01:26,166 --> 00:01:29,418
it takes place by looking at
samples from distribution.

23
00:01:29,418 --> 00:01:33,950
For instance rolling a die
three times might give you 1,

24
00:01:33,950 --> 00:01:37,266
2 and 6, the mean value is then 4.5.

25
00:01:37,266 --> 00:01:42,701
The expected value is the probability
from the underlying distribution is

26
00:01:42,701 --> 00:01:47,910
what would be the mean of a die roll
if we did an infinite number of rolls.

27
00:01:47,910 --> 00:01:53,660
The result is 3.5 since each face of
the die is equally likely to appear.

28
00:01:53,660 --> 00:01:56,089
Thus the expected value is 3.5,

29
00:01:56,089 --> 00:02:00,620
while the mean value depends upon
the samples that we've taken, and

30
00:02:00,620 --> 00:02:05,260
converges to the expected value given
a sufficiently large sample set.

31
00:02:05,260 --> 00:02:08,080
We'll talk more about expected
values in course three.

32
00:02:08,080 --> 00:02:10,600
A second property

33
00:02:10,600 --> 00:02:15,490
is that the variance of the distribution
can be described in a certain way.

34
00:02:15,490 --> 00:02:22,152
Variance is a measure of how badly values
of samples are spread out from the mean.

35
00:02:22,152 --> 00:02:26,376
Let's get a little bit more formal
about five different characteristics of

36
00:02:26,376 --> 00:02:27,368
distributions.

37
00:02:27,368 --> 00:02:31,453
First, we can talk about
the distribution central tendency,

38
00:02:31,453 --> 00:02:36,510
and the measures we would use for
this are mode, median, or mean.

39
00:02:36,510 --> 00:02:40,360
This characteristic is really about
where the bulk of probability

40
00:02:40,360 --> 00:02:41,729
is in the distribution..

41
00:02:42,900 --> 00:02:46,030
We can also talk about
the variability in the distribution.

42
00:02:46,030 --> 00:02:49,070
There are a couple of ways
we can speak of this.

43
00:02:49,070 --> 00:02:52,660
The standard deviation is one,
the interquartile range is another.

44
00:02:52,660 --> 00:02:57,500
The standard deviation is simply
a measure of how different each item,

45
00:02:57,500 --> 00:02:59,530
in our sample, is from the mean.

46
00:03:00,720 --> 00:03:02,520
Here's the formula for standard deviation.

47
00:03:04,040 --> 00:03:07,810
It might look a little more
intimidating than it actually is.

48
00:03:07,810 --> 00:03:10,296
Let's just walk through how
we would write this up.

49
00:03:10,296 --> 00:03:14,806
Let's draw 1,000 samples from
a normal distribution with

50
00:03:14,806 --> 00:03:19,156
an expected value of 0.75 and
a standard deviation of 1.

51
00:03:19,156 --> 00:03:23,840
Then we calculate the actual
mean using NumPy's mean feature.

52
00:03:23,840 --> 00:03:28,535
The part inside the summation
says xi- x bar.

53
00:03:28,535 --> 00:03:32,620
Xi is the current item in the list and
x bar is the mean.

54
00:03:32,620 --> 00:03:35,660
So we calculate the difference,
then we square the result,

55
00:03:35,660 --> 00:03:37,090
then we sum all of these.

56
00:03:38,130 --> 00:03:42,250
This might be a reasonable place to use
a map and apply a lambda to calculate

57
00:03:42,250 --> 00:03:45,480
the differences between the mean and
the measured value.

58
00:03:45,480 --> 00:03:49,620
Then to convert this back to a list,
so NumPy can use it.

59
00:03:49,620 --> 00:03:55,140
Now we just have to square each value, sum
them together, and take the square root.

60
00:03:55,140 --> 00:03:58,087
So that's the size of
our standard deviation.

61
00:03:58,087 --> 00:04:01,199
It covers roughly 68% of
the area around the mean,

62
00:04:01,199 --> 00:04:04,560
split evenly around the side of the mean.

63
00:04:04,560 --> 00:04:08,057
Now we don't normally have to
do all this work ourselves, but

64
00:04:08,057 --> 00:04:11,556
I wanted to show you how you can
sample from the distribution,

65
00:04:11,556 --> 00:04:16,346
create a precise programmatic description
of a formula, and apply it to your data.

66
00:04:16,346 --> 00:04:21,121
But for standard deviation, which is just
one particular measure of variability,

67
00:04:21,121 --> 00:04:24,763
NumPy has a built-in function
that you can apply, called STD.

68
00:04:24,763 --> 00:04:28,805
There's a couple more measures of
distribution that are interesting to

69
00:04:28,805 --> 00:04:30,270
talk about.

70
00:04:30,270 --> 00:04:33,590
One of these is the shape of
the tales of the distribution and

71
00:04:33,590 --> 00:04:36,070
this is called the kurtosis.

72
00:04:36,070 --> 00:04:41,050
We can measure the kurtosis using the
statistics functions in the SciPy package.

73
00:04:41,050 --> 00:04:43,720
A negative value means
the curve is slightly more flat

74
00:04:43,720 --> 00:04:47,460
than a normal distribution, and
a positive value means the curve is

75
00:04:47,460 --> 00:04:50,760
slightly more peaky than
a normal distribution.

76
00:04:50,760 --> 00:04:55,160
Remember that we aren't measuring the
kurtosis of the distribution per se, but

77
00:04:55,160 --> 00:04:59,280
of the thousand values which we
sampled out of the distribution.

78
00:04:59,280 --> 00:05:01,600
This is a sublet but
important distinction.

79
00:05:03,340 --> 00:05:05,890
We could also move out of
the normal distributions and

80
00:05:05,890 --> 00:05:08,670
push the peak of the curve one way or
the other.

81
00:05:08,670 --> 00:05:09,740
And this is called the skew.

82
00:05:10,760 --> 00:05:14,736
If we test our current sample data,
we see that there isn't much of a skew.

83
00:05:14,736 --> 00:05:19,619
Let's switch distributions and take a look
at a distribution called the Chi Squared

84
00:05:19,619 --> 00:05:23,419
distribution, which is also quite
commonly used in statistic.

85
00:05:23,419 --> 00:05:28,360
The Chi Squared Distribution has only one
parameter called the degrees of freedom.

86
00:05:28,360 --> 00:05:32,740
The degrees of freedom is closely related
to the number of samples that you take

87
00:05:32,740 --> 00:05:34,640
from a normal population.

88
00:05:34,640 --> 00:05:37,370
It's important for significance testing.

89
00:05:37,370 --> 00:05:41,870
But what I would like you to observe, is
that as the degrees of freedom increases,

90
00:05:41,870 --> 00:05:45,760
the shape of the Chi Squared
distribution changes.

91
00:05:45,760 --> 00:05:50,570
In particular, the skew to the left
begins to move towards the center.

92
00:05:50,570 --> 00:05:52,580
We can observe this through simulation.

93
00:05:53,630 --> 00:05:57,540
First we'll sample 1,000 values from
a Chi Squared distribution with degrees of

94
00:05:57,540 --> 00:05:59,010
freedom 2.

95
00:05:59,010 --> 00:06:01,961
Now we can see that
the skew is quite large.

96
00:06:01,961 --> 00:06:05,604
Now if we re-sample changing
degrees of freedom to 5.

97
00:06:08,687 --> 00:06:10,960
We see that the skew has
decreased significantly.

98
00:06:12,030 --> 00:06:15,560
We can actually plot this
right in the Jupiter notebook.

99
00:06:15,560 --> 00:06:18,880
I'm not going to talk much about
the library we're using here for plotting,

100
00:06:18,880 --> 00:06:20,810
because that's the topic
of the next course.

101
00:06:21,890 --> 00:06:25,580
But you can see a histogram with our plot
with the two degrees of freedom is skewed

102
00:06:25,580 --> 00:06:27,140
much further to the left,

103
00:06:27,140 --> 00:06:30,430
while our plot with the five degrees
of freedom is not as highly skewed.

104
00:06:31,490 --> 00:06:34,500
I could encourage you as always
to play with this notebook and

105
00:06:34,500 --> 00:06:35,820
change the parameters and

106
00:06:35,820 --> 00:06:39,819
see how the degrees of freedom
changes the skew of the distribution.

107
00:06:40,960 --> 00:06:45,340
The last aspect of distributions that
I want to talk about is the modality.

108
00:06:45,340 --> 00:06:50,460
So far, all of the distributions I've
shown have a single high point, a peak.

109
00:06:50,460 --> 00:06:52,930
But what if we have multiple peaks?

110
00:06:52,930 --> 00:06:56,925
This distribution has two high points,
so we call it bimodal.

111
00:06:56,925 --> 00:07:01,698
These are really interesting distributions
and happen regularly in data mining.

112
00:07:01,698 --> 00:07:04,770
We're going to talk a bit more
about them in course three.

113
00:07:04,770 --> 00:07:08,110
But a useful insight is that we
can actually model these using

114
00:07:08,110 --> 00:07:11,720
two normal distributions
with different parameters.

115
00:07:11,720 --> 00:07:14,210
These are called
Gaussian Mixture Models and

116
00:07:14,210 --> 00:07:16,840
are particularly useful
when clustering data.

117
00:07:18,310 --> 00:07:21,470
Well, this has been a long lecture but
I think it can be tough to chop

118
00:07:21,470 --> 00:07:26,000
up a discussion of distributions and
still get the main points across.

119
00:07:26,000 --> 00:07:30,770
Remember that a distribution is just
a shape that describes the probability

120
00:07:30,770 --> 00:07:34,730
of a value being pulled when
we sample a population.

121
00:07:34,730 --> 00:07:39,690
And NumPy and SciPy each have a number
of different distributions built in for

122
00:07:39,690 --> 00:07:41,060
us to be able to sample from.

123
00:07:42,510 --> 00:07:45,080
The last point I want to leave
you with here is a reference.

124
00:07:45,080 --> 00:07:49,210
If you find this way of exploring
statistics interesting.

125
00:07:49,210 --> 00:07:53,810
Alan Downey wrote a nice book called
Think Stats by the publisher O'Reilly.

126
00:07:53,810 --> 00:07:57,230
I think he does a really nice job of
teaching how to think about statistics

127
00:07:57,230 --> 00:07:59,520
from a programming perspective.

128
00:07:59,520 --> 00:08:03,360
One where you write the functions
behind the statistical methods.

129
00:08:03,360 --> 00:08:04,920
It's not really a reference book, but

130
00:08:04,920 --> 00:08:08,240
it's an interesting way to approach
learning the fundamentals of statistics.

131
00:08:09,470 --> 00:08:13,730
Allen even has a free copy of this book
available on his website in PDF format

132
00:08:13,730 --> 00:08:16,330
and, of course,
all of the code is done in Python.