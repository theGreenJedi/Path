1
00:00:00,000 --> 00:00:05,883
In this video, I am going to describe how 
we might be able to combine recognition 

2
00:00:05,883 --> 00:00:11,476
of objects by using the relationships 
between their parts and recognition of 

3
00:00:11,476 --> 00:00:16,851
objects using a deep neural network. 
At present in computer vision, there's 

4
00:00:16,851 --> 00:00:20,337
broadly three approaches to recognizing 
objects. 

5
00:00:20,337 --> 00:00:26,003
Either you use a deep convolutional 
neural net and this currently works best. 

6
00:00:26,003 --> 00:00:31,886
Or you use a parts-based approach, which 
I think in the long run is going to work 

7
00:00:31,886 --> 00:00:36,803
best,. Or you use the existing features 
that computer vision people know how to 

8
00:00:36,803 --> 00:00:41,419
extract from images and make histograms 
of them and then use lots of hand 

9
00:00:41,419 --> 00:00:44,038
engineering. 
This is the approach that the 

10
00:00:44,038 --> 00:00:47,220
convolutional neural networks have 
recently beaten. 

11
00:00:47,220 --> 00:00:53,065
The point of this video is to show how we 
might combine a parts-based approach with 

12
00:00:53,065 --> 00:00:56,892
early stages that use convolutional 
neraul networks. 

13
00:00:56,892 --> 00:01:02,667
Even though convolutional neural networks 
have worked very well for recognizing 

14
00:01:02,667 --> 00:01:06,288
objects and images, 
I think there's something missing. 

15
00:01:06,288 --> 00:01:11,968
When we pool the activities of a bunch of 
replicator feature detectors, we lose the 

16
00:01:11,968 --> 00:01:16,160
precise position of the feature detector 
that was most active. 

17
00:01:16,160 --> 00:01:21,033
This means we don't know exactly where 
things are and that's fatal for high 

18
00:01:21,033 --> 00:01:23,791
level parts, such as the nose and the 
mouth. 

19
00:01:23,791 --> 00:01:28,600
In order to recognize whose face it is, 
you need to use the precise spatial 

20
00:01:28,600 --> 00:01:32,512
relationships between high level parts, 
like noses and mouths. 

21
00:01:32,512 --> 00:01:37,257
If you overlap the pools so that each 
feature occurs in several different 

22
00:01:37,257 --> 00:01:42,516
pools, you retain more information about 
its position and that makes things work a 

23
00:01:42,516 --> 00:01:45,530
bit better, but I don't think that's the 
answer. 

24
00:01:45,530 --> 00:01:50,290
A related problem is that convolutional 
neural nets that use translations to 

25
00:01:50,290 --> 00:01:54,617
replicate feature detectors cannot 
extrapolate their understanding of 

26
00:01:54,617 --> 00:01:58,883
geometrical relationships to radically 
new viewpoints like different 

27
00:01:58,883 --> 00:02:03,086
orientations or different scales. 
We could, of course, try replicating 

28
00:02:03,086 --> 00:02:08,032
across orientation and scale, but then, 
we get huge numbers of replicated feature 

29
00:02:08,032 --> 00:02:10,381
detectors. 
Now, people are very good at 

30
00:02:10,381 --> 00:02:13,658
extrapolating. 
After seeing a new shape once, they can 

31
00:02:13,658 --> 00:02:16,440
recognize it from a very different 
viewpoint. 

32
00:02:16,440 --> 00:02:22,004
Currently, the way we deal with that with 
convolutional neural networks is to train 

33
00:02:22,004 --> 00:02:25,876
them on transformed data. 
So this involves giving them huge 

34
00:02:25,876 --> 00:02:30,527
training sets, where we tried to 
transform the data through different 

35
00:02:30,527 --> 00:02:35,695
orientations, and scales, and lightings, 
and all sorts of other things so that the 

36
00:02:35,695 --> 00:02:40,799
network can cope with those variations. 
But that's a very clumsy way of dealing 

37
00:02:40,799 --> 00:02:44,546
with the variations. 
I think a much better way is to use a 

38
00:02:44,546 --> 00:02:49,584
hierarchy of coordinate frames and to use 
a group of neurons to represent the 

39
00:02:49,584 --> 00:02:54,430
conjunction of the shape of a feature and 
it's pose relative to the retina. 

40
00:02:54,430 --> 00:02:59,034
So when these neurons are active, it 
tells you a feature of that kind is 

41
00:02:59,034 --> 00:03:02,680
there, like a nose. 
And the precise activities or relative 

42
00:03:02,680 --> 00:03:06,390
activities of these neurons tell you the 
pose of the nose. 

43
00:03:06,390 --> 00:03:11,042
If you think about representing the pose 
of something, that's really a 

44
00:03:11,042 --> 00:03:13,900
relationship between two coordinate 
frames. 

45
00:03:13,900 --> 00:03:17,892
So it's a relationship between a 
coordinate frame embedded in the thing 

46
00:03:17,892 --> 00:03:20,610
and the coordinate frame of the camera or 
retina. 

47
00:03:20,610 --> 00:03:26,072
So, in order to represent the pose of 
something, we have to embed a coordinate 

48
00:03:26,072 --> 00:03:29,737
frame within it. 
Once we've done this and we have a 

49
00:03:29,737 --> 00:03:35,487
representation of the pose of thoughts of 
objects relative to the reckoner, it's 

50
00:03:35,487 --> 00:03:40,590
easy to use the relationships between 
paths to recognize larger objects. 

51
00:03:40,590 --> 00:03:45,901
So we're going to use the consistency of 
the poses of the parts as a cue for 

52
00:03:45,901 --> 00:03:50,567
recognizing a larger shape. 
If you look at this picture, we have a 

53
00:03:50,567 --> 00:03:56,165
nose and we have a mouth and then the 
right spacial relationship to one 

54
00:03:56,165 --> 00:03:59,218
another. 
One way of thinking about that is that if 

55
00:03:59,218 --> 00:04:03,972
you ask the mouth to predict the pose of 
the whole face, and if you ask the nose 

56
00:04:03,972 --> 00:04:08,190
to predict the pose of the whole face, 
they'll make similar predictions. 

57
00:04:08,190 --> 00:04:12,944
If you look on the right share, we have 
the same nose and the same mouth, but now 

58
00:04:12,944 --> 00:04:15,440
they're in the wrong spatial 
relationship. 

59
00:04:15,440 --> 00:04:21,169
And that means that if they separately 
make predictions about the pose of the 

60
00:04:21,169 --> 00:04:24,695
whole face, those predictions won't agree 
at all. 

61
00:04:24,695 --> 00:04:30,204
So here's two layers in a hierarchy of 
parts, where the larger parts can be 

62
00:04:30,204 --> 00:04:34,318
recognized by consistent predictions from 
smaller parts. 

63
00:04:34,318 --> 00:04:39,680
Let's suppose we're looking for a face. 
And so in the middle here the ellipse 

64
00:04:39,680 --> 00:04:45,557
with Tj in it is a collection of neurons 
that are going to be used for recognizing 

65
00:04:45,557 --> 00:04:49,705
the pose of the face. 
And the Pj next to it is a single 

66
00:04:49,705 --> 00:04:54,624
logistic neuron that's going to be used 
for representing whether or not we think 

67
00:04:54,624 --> 00:04:58,764
there's a face there. 
We have a similar representation in the 

68
00:04:58,764 --> 00:05:03,067
lab below, where we have a representation 
of the pose for a mouth and a 

69
00:05:03,067 --> 00:05:08,062
representation of the pose for a nose, 
and we can recognize the phase by 

70
00:05:08,062 --> 00:05:13,448
noticing that those two representations 
make consistent predictions. 

71
00:05:13,448 --> 00:05:19,151
So we take a vector of activities that 
represents the pose of the mouth. 

72
00:05:19,151 --> 00:05:26,199
We multiply by matrix Tij that represents 
the spatial relationship between a mouth 

73
00:05:26,199 --> 00:05:32,780
and a face, and we get a prediction, Ti * 
Tij for the pose of the face. 

74
00:05:32,780 --> 00:05:37,135
We do the same thing with the nose. 
We take a vector of neural activities 

75
00:05:37,135 --> 00:05:41,609
that represents the pose of the nose, Th. 
We multiply it by the relationship 

76
00:05:41,609 --> 00:05:46,382
between the nose and the face and we get 
another prediction for the pose of the 

77
00:05:46,382 --> 00:05:49,126
face. 
If those two predictions agree, there's a 

78
00:05:49,126 --> 00:05:53,124
face there, because the nose and the 
mouth are in the right spacial 

79
00:05:53,124 --> 00:05:57,360
relationship and that's very unlikely 
without there being a face there. 

80
00:05:57,360 --> 00:06:00,834
What we're doing here is inverse computer 
graphics. 

81
00:06:00,834 --> 00:06:06,011
In computer graphics, if you knew the 
pose of the face, you could now compute 

82
00:06:06,011 --> 00:06:11,256
by using the inverse of Tij, the pose of 
the mouth and similarly for the nose. 

83
00:06:11,256 --> 00:06:16,910
So in computer graphics you're going from 
poses of larger things to poses of their 

84
00:06:16,910 --> 00:06:19,975
parts. 
In computer vision, you need to go from 

85
00:06:19,975 --> 00:06:25,084
the poses of the parts to the poses of 
larger things and you need to check 

86
00:06:25,084 --> 00:06:29,579
consistency when you do that. 
Now if we can get a neural net to 

87
00:06:29,579 --> 00:06:35,359
represent these pose vectors, as vectors 
of neural activity, then we get a very 

88
00:06:35,359 --> 00:06:38,916
nice property. 
Spatial relationships can then be 

89
00:06:38,916 --> 00:06:43,634
modelled as linear operations. 
That makes it very easy to learn 

90
00:06:43,634 --> 00:06:48,312
hierarchy visual entities, 
and it also makes it very easy to 

91
00:06:48,312 --> 00:06:53,498
generalize across viewpoints. 
So, what's going to happen when we make 

92
00:06:53,498 --> 00:06:59,163
small changes in viewpoint is the pose 
vectors, those vectors of neural 

93
00:06:59,163 --> 00:07:04,588
activities are all going to change. 
What's going to be invariant is the 

94
00:07:04,588 --> 00:07:08,418
weights. 
It was the weights that represented the 

95
00:07:08,418 --> 00:07:12,726
relationship between the part from the 
hole, like Tij on the previous slide, and 

96
00:07:12,726 --> 00:07:17,671
those don't depend on viewpoint. 
So we want to get the invariant 

97
00:07:17,671 --> 00:07:22,737
properties of a shape into the weights. 
And we want to have the pose vectors in 

98
00:07:22,737 --> 00:07:26,071
the activities, 
because when we change viewpoint, all 

99
00:07:26,071 --> 00:07:30,764
those pose vectors are going to change. 
So, rather than trying to get neural 

100
00:07:30,764 --> 00:07:35,533
activities that are invariant to 
viewpoint, which is what the pooling in 

101
00:07:35,533 --> 00:07:40,104
the convolutional net is trying to do. 
We're going to aim to get neural 

102
00:07:40,104 --> 00:07:43,084
activities that are equivariant of 
viewpoint. 

103
00:07:43,084 --> 00:07:47,588
As the pose of the object varies the 
activities of the neurons vary. 

104
00:07:47,588 --> 00:07:52,622
That means the percept of an object, not 
its label but what it looks like, is 

105
00:07:52,622 --> 00:07:57,981
going to change as the viewpoint changes. 
I'm going to finish by giving you some 

106
00:07:57,981 --> 00:08:03,811
evidence that our visual systems really 
do impose coordinate frames in order to 

107
00:08:03,811 --> 00:08:07,440
represent shapes. 
This was pointed out a long time ago by a 

108
00:08:07,440 --> 00:08:11,824
great psychologist called Irvin Rock. 
So if you look at this shape and I tell 

109
00:08:11,824 --> 00:08:15,353
you it's a country, most people don't 
know which country it is. 

110
00:08:15,353 --> 00:08:19,679
They look at it, and they think it looks 
a bit like Australia, but it's so sort, 

111
00:08:19,679 --> 00:08:24,005
sort of mirror image of Australia, 
but it's not really a familiar country at 

112
00:08:24,005 --> 00:08:26,453
all. 
If you tell them where the top is, that 

113
00:08:26,453 --> 00:08:30,460
it's that way up, they immediately 
recognize that's it's Africa. 

114
00:08:30,460 --> 00:08:35,138
If they knew what coordinate frame to 
impose on it, it immediately becomes a 

115
00:08:35,138 --> 00:08:38,339
familiar shape. 
Similarly, if I give you a shape like 

116
00:08:38,339 --> 00:08:43,820
this one, you can perceive it as a square 
or you can perceive it as a diamond. 

117
00:08:43,820 --> 00:08:46,518
Those are two completely different 
percepts. 

118
00:08:46,518 --> 00:08:50,934
What you know about the shape is totally 
different depending on which way you 

119
00:08:50,934 --> 00:08:55,090
perceive it. 
So for example, if you perceive it as a 

120
00:08:55,090 --> 00:09:01,930
tilted square, you're acutely sensitive 
to whether the angles are right angles. 

121
00:09:01,930 --> 00:09:06,074
If you perceive it as an upright diamond, 
you're not sensitive to that at all. 

122
00:09:06,074 --> 00:09:10,704
The angles could be 5 degrees off and you 
wouldn't notice it, but you are sensitive 

123
00:09:10,704 --> 00:09:13,880
to something else. 
If you perceive it as an upright diamond, 

124
00:09:13,880 --> 00:09:18,240
you're acutely sensitive to whether the 
corner on the left and the corner of the 

125
00:09:18,240 --> 00:09:21,900
right are at the same height. 
And you'll probably notice now that, in 

126
00:09:21,900 --> 00:09:24,700
this figure, they're very slightly 
different heights. 

127
00:09:24,700 --> 00:09:28,478
These kind of demonstrations are 
evidence, that in order to represent 

128
00:09:28,478 --> 00:09:30,833
shapes we impose coordinate frames on 
them. 

129
00:09:30,833 --> 00:09:34,885
Because when you're looking at that 
square or diamond, it's the same thing 

130
00:09:34,885 --> 00:09:38,828
you're looking at, but the percept's 
totally different depending on what 

131
00:09:38,828 --> 00:09:40,362
coordinate frame you impose. 